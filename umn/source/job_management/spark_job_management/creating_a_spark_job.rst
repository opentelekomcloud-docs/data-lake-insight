:original_name: dli_01_0384.html

.. _dli_01_0384:

Creating a Spark Job
====================

DLI provides fully-managed Spark computing services by allowing you to execute Spark jobs.

On the **Overview** page, click **Create Job** in the upper right corner of the **Spark Jobs** tab or click **Create Job** in the upper right corner of the **Spark Jobs** page. The Spark job editing page is displayed.

On the Spark job editing page, a message is displayed, indicating that a temporary DLI data bucket will be created. The created bucket is used to store temporary data generated by DLI, such as job logs and job results. You cannot view job logs if you choose not to create it. The bucket will be created and the default bucket name is used.

If you do not need to create a DLI temporary data bucket and do not want to receive this message, select **Do not show again** and click **Cancel**.

Prerequisites
-------------

-  You have uploaded the dependencies to the corresponding OBS bucket on the **Data Management > Package Management** page. For details, see :ref:`Creating a Package <dli_01_0367>`.
-  Before creating a Spark job to access other external data sources, such as OpenTSDB, HBase, Kafka, GaussDB(DWS), RDS, CSS, CloudTable, DCS Redis, and DDS, you need to create a cross-source connection to enable the network between the job running queue and external data sources.

   -  For details about the external data sources that can be accessed by Spark jobs, see :ref:`Cross-Source Analysis Development Methods <dli_01_0410>`.

   -  For details about how to create a datasource connection, see :ref:`Enhanced Datasource Connections <dli_01_0426>`.

      On the **Resources** > **Queue Management** page, locate the queue you have created, and choose **More** > **Test Address Connectivity** in the **Operation** column to check whether the network connection between the queue and the data source is normal. For details, see :ref:`Testing Address Connectivity <dli_01_0489>`.

Procedure
---------

#. In the left navigation pane of the DLI management console, choose **Job Management** > **Spark Jobs**. The **Spark Jobs** page is displayed.

   Click **Create Job** in the upper right corner. In the job editing window, you can set parameters in **Fill Form** mode or **Write API** mode.

   The following uses the **Fill Form** as an example. In **Write API** mode, refer to the *Data Lake Insight API Reference* for parameter settings.

2. Select a queue.

   Select the queue you want to use from the drop-down list box.

3. Configure the job.

   Configure job parameters by referring to :ref:`Table 1 <dli_01_0384__table15251951125017>`.

   .. _dli_01_0384__table15251951125017:

   .. table:: **Table 1** Job configuration parameters

      +---------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
      | Parameter                             | Description                                                                                                                                                                                                                                                                                                   |
      +=======================================+===============================================================================================================================================================================================================================================================================================================+
      | Job Name (--name)                     | Set a job name.                                                                                                                                                                                                                                                                                               |
      +---------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
      | Application                           | Select the package to be executed. The value can be **.jar** or **.py**.                                                                                                                                                                                                                                      |
      |                                       |                                                                                                                                                                                                                                                                                                               |
      |                                       | You can select the name of a JAR or pyFile package that has been uploaded to the DLI resource management system. You can also specify an OBS path, for example, **obs://**\ *Bucket name*\ **/**\ *Package name*.                                                                                             |
      +---------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
      | Main Class (--class)                  | Enter the name of the main class. When the application type is **.jar**, the main class name cannot be empty.                                                                                                                                                                                                 |
      +---------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
      | Spark Arguments (--conf)              | Enter a parameter in the format of **key=value**. Press Enter to separate multiple key-value pairs.                                                                                                                                                                                                           |
      |                                       |                                                                                                                                                                                                                                                                                                               |
      |                                       | These parameters can be replaced with global variables. For example, if you create a global variable **custom_class** on the **Global Configuration** > **Global Variables** page, you can use **"spark.sql.catalog"={{custom_class}}** to replace a parameter with this variable after the job is submitted. |
      |                                       |                                                                                                                                                                                                                                                                                                               |
      |                                       | .. note::                                                                                                                                                                                                                                                                                                     |
      |                                       |                                                                                                                                                                                                                                                                                                               |
      |                                       |    -  The JVM garbage collection algorithm cannot be customized for Spark jobs.                                                                                                                                                                                                                               |
      |                                       |    -  If the Spark version is **3.1.1**, configure **Spark parameters (--conf)** to select a dependent module. For details about the example configuration, see :ref:`Table 2 <dli_01_0384__table17326112613420>`.                                                                                            |
      +---------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
      | Job Type                              | Set this parameter when you select a CCE queue. Type of the Spark image used by a job. The options are as follows:                                                                                                                                                                                            |
      |                                       |                                                                                                                                                                                                                                                                                                               |
      |                                       | -  **Basic**: Basic images provided by DLI. Select this option for non-AI jobs.                                                                                                                                                                                                                               |
      |                                       | -  **AI-enhanced**: AI images provided by DLI. Select this option for AI jobs.                                                                                                                                                                                                                                |
      |                                       | -  **Image**: Custom Spark images. Select an existing image name and version on SWR.                                                                                                                                                                                                                          |
      +---------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
      | JAR Package Dependencies (--jars)     | JAR file on which the Spark job depends. You can enter the JAR package name or the corresponding OBS path. The format is as follows: **obs://Bucket name/Folder name/Package name**.                                                                                                                          |
      +---------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
      | Python File Dependencies (--py-files) | py-files on which the Spark job depends. You can enter the Python file name or the corresponding OBS path of the Python file. The format is as follows: **obs://Bucket name/Folder name/File name**.                                                                                                          |
      +---------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
      | Other Dependencies (--files)          | Other files on which the Spark job depends. You can enter the name of the dependency file or the corresponding OBS path of the dependency file. The format is as follows: **obs://Bucket name/Folder name/File name**.                                                                                        |
      +---------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
      | Group Name                            | If you select a group when creating a package, you can select all the packages and files in the group. For details about how to create a package, see :ref:`Creating a Package <dli_01_0367>`.                                                                                                                |
      +---------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
      | Access Metadata                       | Whether to access metadata through Spark jobs. .                                                                                                                                                                                                                                                              |
      +---------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
      | Retry upon Failure                    | Indicates whether to retry a failed job.                                                                                                                                                                                                                                                                      |
      |                                       |                                                                                                                                                                                                                                                                                                               |
      |                                       | If you select **Yes**, you need to set the following parameters:                                                                                                                                                                                                                                              |
      |                                       |                                                                                                                                                                                                                                                                                                               |
      |                                       | **Maximum Retries**: Maximum number of retry times. The maximum value is **100**.                                                                                                                                                                                                                             |
      +---------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
      | Advanced Settings                     | -  Skip                                                                                                                                                                                                                                                                                                       |
      |                                       | -  Configure                                                                                                                                                                                                                                                                                                  |
      |                                       |                                                                                                                                                                                                                                                                                                               |
      |                                       |    -  **Select Dependency Resources**: For details about the parameters, see :ref:`Table 3 <dli_01_0384__table6886411195318>`.                                                                                                                                                                                |
      |                                       |    -  **Configure Resources**: For details about the parameters, see :ref:`Table 4 <dli_01_0384__table388781175311>`.                                                                                                                                                                                         |
      +---------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

   .. _dli_01_0384__table17326112613420:

   .. table:: **Table 2** Spark Parameter (--conf) configuration

      +-----------------------------------+-----------------------------------------------------------------------------------------+
      | Datasource                        | Example Value                                                                           |
      +===================================+=========================================================================================+
      | CSS                               | spark.driver.extraClassPath=/usr/share/extension/dli/spark-jar/datasource/css/\*        |
      |                                   |                                                                                         |
      |                                   | spark.executor.extraClassPath=/usr/share/extension/dli/spark-jar/datasource/css/\*      |
      +-----------------------------------+-----------------------------------------------------------------------------------------+
      | DWS                               | spark.driver.extraClassPath=/usr/share/extension/dli/spark-jar/datasource/dws/\*        |
      |                                   |                                                                                         |
      |                                   | spark.executor.extraClassPath=/usr/share/extension/dli/spark-jar/datasource/dws/\*      |
      +-----------------------------------+-----------------------------------------------------------------------------------------+
      | HBase                             | spark.driver.extraClassPath=/usr/share/extension/dli/spark-jar/datasource/hbase/\*      |
      |                                   |                                                                                         |
      |                                   | spark.executor.extraClassPath=/usr/share/extension/dli/spark-jar/datasource/hbase/\*    |
      +-----------------------------------+-----------------------------------------------------------------------------------------+
      | OpenTSDB                          | park.driver.extraClassPath=/usr/share/extension/dli/spark-jar/datasource/opentsdb/\*    |
      |                                   |                                                                                         |
      |                                   | spark.executor.extraClassPath=/usr/share/extension/dli/spark-jar/datasource/opentsdb/\* |
      +-----------------------------------+-----------------------------------------------------------------------------------------+
      | RDS                               | spark.driver.extraClassPath=/usr/share/extension/dli/spark-jar/datasource/rds/\*        |
      |                                   |                                                                                         |
      |                                   | spark.executor.extraClassPath=/usr/share/extension/dli/spark-jar/datasource/rds/\*      |
      +-----------------------------------+-----------------------------------------------------------------------------------------+
      | Redis                             | spark.driver.extraClassPath=/usr/share/extension/dli/spark-jar/datasource/redis/\*      |
      |                                   |                                                                                         |
      |                                   | spark.executor.extraClassPath=/usr/share/extension/dli/spark-jar/datasource/redis/\*    |
      +-----------------------------------+-----------------------------------------------------------------------------------------+

4. Set the following parameters in advanced settings:

   -  **Select Dependency Resources**: For details about the parameters, see :ref:`Table 3 <dli_01_0384__table6886411195318>`.
   -  **Configure Resources**: For details about the parameters, see :ref:`Table 4 <dli_01_0384__table388781175311>`.

      .. note::

         The parallelism degree of Spark resources is jointly determined by the number of Executors and the number of Executor CPU cores.

         **Maximum number of tasks that can be concurrently executed = Number of Executors x Number of Executor CPU cores**

         You can properly plan compute resource specifications based on the compute CUs of the queue you have purchased.

         Note that Spark tasks need to be jointly executed by multiple roles, such as **driver** and **executor**. So, the number of executors multiplied by the number of executor CPU cores must be less than the number of compute CUs of the queue to prevent other roles from failing to start Spark tasks. For more information about roles for Spark tasks, see `Apache Spark <https://spark.apache.org/>`__.

         **Calculation formula for Spark job parameters:**

         -  CUs = Driver Cores + Executors x Executor Cores
         -  Memory = Driver Memory + (Executors x Executor Memory)

   .. _dli_01_0384__table6886411195318:

   .. table:: **Table 3** Parameters for selecting dependency resources

      +-----------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------+
      | Parameter                         | Description                                                                                                                                      |
      +===================================+==================================================================================================================================================+
      | modules                           | If the Spark version is **3.1.1**, you do not need to select a module. Configure **Spark parameters (--conf)**.                                  |
      |                                   |                                                                                                                                                  |
      |                                   | Dependency modules provided by DLI for executing datasource connection jobs. To access different services, you need to select different modules. |
      |                                   |                                                                                                                                                  |
      |                                   | -  MRS HBase: sys.datasource.hbase                                                                                                               |
      |                                   | -  DDS: sys.datasource.mongo                                                                                                                     |
      |                                   | -  MRS OpenTSDB: sys.datasource.opentsdb                                                                                                         |
      |                                   | -  DWS: sys.datasource.dws                                                                                                                       |
      |                                   | -  RDS MySQL: sys.datasource.rds                                                                                                                 |
      |                                   | -  RDS PostGre: sys.datasource.rds                                                                                                               |
      |                                   | -  DCS: sys.datasource.redis                                                                                                                     |
      |                                   | -  CSS: sys.datasource.css                                                                                                                       |
      +-----------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------+
      | Resource Package                  | JAR package on which the Spark job depends.                                                                                                      |
      +-----------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------+

   .. _dli_01_0384__table388781175311:

   .. table:: **Table 4** Resource specification parameters

      +-----------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------+
      | Parameter                         | Description                                                                                                                                     |
      +===================================+=================================================================================================================================================+
      | Resource Specifications           | Select a resource specification from the drop-down list box. The system provides three resource specification options for you to choose from.   |
      |                                   |                                                                                                                                                 |
      |                                   | Resource specifications involve the following parameters:                                                                                       |
      |                                   |                                                                                                                                                 |
      |                                   | -  Executor Memory                                                                                                                              |
      |                                   | -  Executor Cores                                                                                                                               |
      |                                   | -  Executors                                                                                                                                    |
      |                                   | -  Driver Cores                                                                                                                                 |
      |                                   | -  Driver Memory                                                                                                                                |
      |                                   |                                                                                                                                                 |
      |                                   | If modified, your modified settings of the items are used.                                                                                      |
      +-----------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------+
      | Executor Memory                   | Customize the configuration item based on the selected resource specifications.                                                                 |
      |                                   |                                                                                                                                                 |
      |                                   | Memory of each Executor. It is recommended that the ratio of Executor CPU cores to Executor memory be 1:4.                                      |
      +-----------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------+
      | Executor Cores                    | Number of CPU cores of each Executor applied for by Spark jobs, which determines the capability of each Executor to execute tasks concurrently. |
      +-----------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------+
      | Executors                         | Number of Executors applied for by a Spark job                                                                                                  |
      +-----------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------+
      | Driver Cores                      | Number of CPU cores of the driver                                                                                                               |
      +-----------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------+
      | Driver Memory                     | Driver memory size. It is recommended that the ratio of the number of driver CPU cores to the driver memory be 1:4.                             |
      +-----------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------+

5. Click **Execute** in the upper right corner of the Spark job editing page.

   After the message "Batch processing job submitted successfully" is displayed, you can view the status and logs of the submitted job on the **Spark Jobs** page.
