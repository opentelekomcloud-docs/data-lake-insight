:original_name: dli_03_0217.html

.. _dli_03_0217:

Spark Job Development
=====================

-  :ref:`Spark Jobs <dli_03_0201>`
-  :ref:`How Do I Use Spark to Write Data into a DLI Table? <dli_03_0107>`
-  :ref:`How Do I Set Up AK/SK So That a General Queue Can Access Tables Stored in OBS? <dli_03_0017>`
-  :ref:`How Do I View the Resource Usage of DLI Spark Jobs? <dli_03_0102>`
-  :ref:`How Do I Use Python Scripts to Access the MySQL Database If the pymysql Module Is Missing from the Spark Job Results Stored in MySQL? <dli_03_0076>`
-  :ref:`How Do I Run a Complex PySpark Program in DLI? <dli_03_0082>`
-  :ref:`How Do I Use JDBC to Set the spark.sql.shuffle.partitions Parameter to Improve the Task Concurrency? <dli_03_0068>`
-  :ref:`How Do I Read Uploaded Files for a Spark Jar Job? <dli_03_0118>`
-  :ref:`Why Can't I Find the Specified Python Environment After Adding the Python Package? <dli_03_0077>`
-  :ref:`Why Is a Spark Jar Job Stuck in the Submitting State? <dli_03_0220>`

.. toctree::
   :maxdepth: 1
   :hidden: 

   spark_jobs
   how_do_i_use_spark_to_write_data_into_a_dli_table
   how_do_i_set_up_ak_sk_so_that_a_general_queue_can_access_tables_stored_in_obs
   how_do_i_view_the_resource_usage_of_dli_spark_jobs
   how_do_i_use_python_scripts_to_access_the_mysql_database_if_the_pymysql_module_is_missing_from_the_spark_job_results_stored_in_mysql
   how_do_i_run_a_complex_pyspark_program_in_dli
   how_do_i_use_jdbc_to_set_the_spark.sql.shuffle.partitions_parameter_to_improve_the_task_concurrency
   how_do_i_read_uploaded_files_for_a_spark_jar_job
   why_cant_i_find_the_specified_python_environment_after_adding_the_python_package
   why_is_a_spark_jar_job_stuck_in_the_submitting_state
